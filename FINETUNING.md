# Guia de Fine-tuning para Face Recognition

Este documento apresenta um guia completo para realizar fine-tuning do modelo de Face Recognition treinado no VGGFace2 em um novo conjunto de dados.

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [PrÃ©-requisitos](#prÃ©-requisitos)
3. [Estrutura do Dataset](#estrutura-do-dataset)
4. [RetinaFace na ValidaÃ§Ã£o](#retinaface-na-validaÃ§Ã£o)
5. [EstratÃ©gias de Fine-tuning](#estratÃ©gias-de-fine-tuning)
6. [Passo a Passo](#passo-a-passo)
7. [Salvamento de Modelos](#salvamento-de-modelos)
8. [Monitoramento e AvaliaÃ§Ã£o](#monitoramento-e-avaliaÃ§Ã£o)
9. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ VisÃ£o Geral

O fine-tuning permite adaptar um modelo prÃ©-treinado (treinado no VGGFace2 e validado no LFW) para um novo conjunto de dados especÃ­fico. Este processo Ã© mais eficiente que treinar do zero e geralmente produz melhores resultados com menos dados.

### Quando usar Fine-tuning?

- âœ… VocÃª tem um novo conjunto de dados com classes diferentes do VGGFace2
- âœ… VocÃª tem um conjunto de dados menor (< 100k imagens)
- âœ… VocÃª quer adaptar o modelo para um domÃ­nio especÃ­fico
- âœ… VocÃª quer manter as caracterÃ­sticas aprendidas do modelo original

---

## ğŸ“¦ PrÃ©-requisitos

### 1. Modelo PrÃ©-treinado

VocÃª precisa ter um modelo prÃ©-treinado salvo. O modelo deve ser do tipo `.keras` e conter:
- Backbone ResNet50
- Camada CosFace
- Pesos treinados no VGGFace2

**LocalizaÃ§Ã£o esperada:** O modelo pode estar em qualquer local. Exemplos:
- `experiments/Resnet50_vgg_cropado_CelebA/checkpoints/epoch_XX.keras`
- `models/pretrained_model.keras`

### 2. Novo Dataset

O novo dataset deve estar organizado em uma das seguintes estruturas:

#### OpÃ§Ã£o A: Dataset com divisÃ£o train/val
```
/dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ pessoa_001/
â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â”œâ”€â”€ img2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ pessoa_002/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â””â”€â”€ val/
    â”œâ”€â”€ pessoa_001/
    â”œâ”€â”€ pessoa_002/
    â””â”€â”€ ...
```

#### OpÃ§Ã£o B: Dataset Ãºnico (serÃ¡ dividido automaticamente)
```
/dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/
â”œâ”€â”€ pessoa_001/
â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”œâ”€â”€ img2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ pessoa_002/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

**Requisitos:**
- Imagens devem estar alinhadas e redimensionadas para 112x112 pixels
- Formato: JPG ou PNG
- Cada pasta representa uma classe (pessoa)
- MÃ­nimo recomendado: 10 imagens por classe

### 3. DependÃªncias

Certifique-se de que todas as dependÃªncias estÃ£o instaladas:

```bash
pip install -r requirements.txt
```

**DependÃªncias importantes:**
- TensorFlow/Keras
- RetinaFace (via `uniface`) - usado automaticamente na validaÃ§Ã£o
- OpenCV
- scikit-image

---

## ğŸ” RetinaFace na ValidaÃ§Ã£o

### VisÃ£o Geral

O script oferece a opÃ§Ã£o de aplicar o **RetinaFace** na fase de validaÃ§Ã£o para garantir que apenas amostras com detecÃ§Ã£o de rosto sejam utilizadas. Isso melhora a qualidade da validaÃ§Ã£o e evita que imagens sem rosto afetem as mÃ©tricas.

**âš ï¸ Importante:** O RetinaFace Ã© **opcional** e deve ser habilitado explicitamente usando a flag `--use_retinaface`.

### Como Funciona

1. **Durante o Treinamento:**
   - O dataset de treino Ã© processado normalmente, sem filtro de RetinaFace
   - Isso permite que o modelo aprenda com todos os dados disponÃ­veis

2. **Durante a ValidaÃ§Ã£o (quando `--use_retinaface` estÃ¡ habilitado):**
   - Cada imagem do dataset de validaÃ§Ã£o passa pelo RetinaFace
   - Se o RetinaFace **nÃ£o detectar** um rosto, a amostra Ã© **excluÃ­da** automaticamente
   - Apenas amostras com detecÃ§Ã£o de rosto sÃ£o usadas para calcular mÃ©tricas de validaÃ§Ã£o

3. **Quando RetinaFace estÃ¡ desabilitado:**
   - O dataset de validaÃ§Ã£o Ã© usado normalmente, sem filtro
   - Todas as amostras sÃ£o incluÃ­das na validaÃ§Ã£o

### PolÃ­tica de ExclusÃ£o

- âœ… **Amostras com rosto detectado**: IncluÃ­das na validaÃ§Ã£o
- âŒ **Amostras sem rosto detectado**: ExcluÃ­das automaticamente
- âš ï¸ **Erros de detecÃ§Ã£o**: Tratados como "sem detecÃ§Ã£o" e excluÃ­dos

### BenefÃ­cios

- **ValidaÃ§Ã£o mais precisa**: Apenas imagens vÃ¡lidas sÃ£o consideradas
- **MÃ©tricas mais confiÃ¡veis**: Evita ruÃ­do de imagens sem rosto
- **Processo automÃ¡tico**: NÃ£o requer intervenÃ§Ã£o manual
- **Apenas na validaÃ§Ã£o**: NÃ£o afeta o treinamento

### Como Habilitar

Para habilitar o RetinaFace na validaÃ§Ã£o, adicione a flag `--use_retinaface` ao comando:

```bash
python run_finetuning.py \
    --strategy 1 \
    --pretrained_model models/pretrained.keras \
    --dataset_path /dados/datasets/... \
    --output_dir experiments/finetuning_strategy1 \
    --use_retinaface  # <-- Adicione esta flag
```

**Sem a flag:** O RetinaFace nÃ£o Ã© aplicado e todas as amostras de validaÃ§Ã£o sÃ£o usadas.

### Mensagens Durante a ExecuÃ§Ã£o

Quando o RetinaFace estÃ¡ habilitado, vocÃª verÃ¡ mensagens como:

```
Carregando dataset de validaÃ§Ã£o RAW para aplicar RetinaFace...
============================================================
APLICANDO RETINAFACE NA VALIDAÃ‡ÃƒO
============================================================
Filtrando amostras sem detecÃ§Ã£o de rosto...
Filtro RetinaFace aplicado com sucesso na validaÃ§Ã£o.
Amostras sem detecÃ§Ã£o de rosto foram excluÃ­das.
```

Quando desabilitado:
```
RetinaFace desabilitado. Usando dataset de validaÃ§Ã£o padrÃ£o.
```

### Notas Importantes

- âš ï¸ O RetinaFace Ã© **opcional** e deve ser habilitado com `--use_retinaface`
- âš ï¸ O RetinaFace Ã© aplicado **apenas na validaÃ§Ã£o**, nÃ£o no treinamento
- âš ï¸ O processo de filtragem pode reduzir o tamanho do dataset de validaÃ§Ã£o
- âš ï¸ Se muitas amostras forem excluÃ­das, considere revisar a qualidade do dataset
- âœ… O modelo RetinaFace Ã© carregado uma Ãºnica vez e reutilizado (eficiente)
- ğŸ’¡ **RecomendaÃ§Ã£o**: Use `--use_retinaface` quando o dataset pode conter imagens sem rosto ou de baixa qualidade

---

## ğŸ”§ EstratÃ©gias de Fine-tuning

O script oferece 3 estratÃ©gias diferentes, cada uma adequada para diferentes cenÃ¡rios:

### EstratÃ©gia 1: Full Fine-tuning (Fine-tuning Completo)

**Quando usar:**
- VocÃª tem um dataset grande (> 10k imagens)
- O novo dataset Ã© similar ao VGGFace2
- VocÃª quer mÃ¡xima adaptaÃ§Ã£o ao novo domÃ­nio

**CaracterÃ­sticas:**
- Todas as camadas do backbone sÃ£o treinÃ¡veis
- Todas as camadas da cabeÃ§a sÃ£o treinÃ¡veis
- Learning rate: 10% do learning rate original
- Mais flexÃ­vel, mas requer mais dados

**Comando:**
```bash
python run_finetuning.py \
    --strategy 1 \
    --pretrained_model experiments/Resnet50_vgg_cropado_CelebA/checkpoints/epoch_30.keras \
    --dataset_path /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ \
    --output_dir experiments/finetuning_strategy1 \
    --epochs 30 \
    --batch_size 64 \
    --use_retinaface  # Opcional: habilita filtro RetinaFace na validaÃ§Ã£o
```

---

### EstratÃ©gia 2: Partial Fine-tuning (Fine-tuning Parcial)

**Quando usar:**
- VocÃª tem um dataset mÃ©dio (1k - 10k imagens)
- Quer evitar overfitting
- Quer preservar mais caracterÃ­sticas do modelo original

**CaracterÃ­sticas:**
- Apenas as Ãºltimas N camadas do backbone sÃ£o treinÃ¡veis
- Todas as camadas da cabeÃ§a sÃ£o treinÃ¡veis
- Camadas iniciais do backbone permanecem congeladas
- Menos parÃ¢metros treinÃ¡veis = menos risco de overfitting

**ParÃ¢metros:**
- `--num_layers`: NÃºmero de camadas finais a treinar (default: 10)
  - Valores recomendados: 5-20
  - Mais camadas = mais adaptaÃ§Ã£o, mas mais risco de overfitting

**Comando:**
```bash
python run_finetuning.py \
    --strategy 2 \
    --pretrained_model experiments/Resnet50_vgg_cropado_CelebA/checkpoints/epoch_30.keras \
    --dataset_path /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ \
    --output_dir experiments/finetuning_strategy2 \
    --epochs 30 \
    --batch_size 64 \
    --num_layers 15
```

---

### EstratÃ©gia 3: Differential LR Fine-tuning (LR Diferenciado)

**Quando usar:**
- VocÃª quer um equilÃ­brio entre adaptaÃ§Ã£o e preservaÃ§Ã£o
- VocÃª tem experiÃªncia com fine-tuning
- VocÃª quer mÃ¡xima performance

**CaracterÃ­sticas:**
- Todas as camadas sÃ£o treinÃ¡veis
- Learning rates diferenciados por profundidade:
  - Camadas profundas: LR muito baixo (preserva features bÃ¡sicas)
  - Camadas mÃ©dias: LR mÃ©dio
  - Camadas superficiais: LR mais alto (adapta features especÃ­ficas)
  - CabeÃ§a: LR mais alto ainda

**Nota:** A implementaÃ§Ã£o atual usa um LR mÃ©dio. Para LR verdadeiramente diferenciado, considere treinar em fases ou usar uma implementaÃ§Ã£o customizada.

**Comando:**
```bash
python run_finetuning.py \
    --strategy 3 \
    --pretrained_model experiments/Resnet50_vgg_cropado_CelebA/checkpoints/epoch_30.keras \
    --dataset_path /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ \
    --output_dir experiments/finetuning_strategy3 \
    --epochs 30 \
    --batch_size 64
```

---

## ğŸ“ Passo a Passo

### Passo 1: Preparar o Ambiente

```bash
# Navegar para o diretÃ³rio do projeto
cd /Users/wgalvao/Noleak/Implementacao-Cosface

# Verificar se o modelo prÃ©-treinado existe
ls -lh experiments/Resnet50_vgg_cropado_CelebA/checkpoints/

# Verificar se o dataset estÃ¡ acessÃ­vel
ls -lh /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/
```

### Passo 2: Verificar o Dataset

```bash
# Contar nÃºmero de classes
find /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ -type d -mindepth 1 -maxdepth 1 | wc -l

# Verificar estrutura (se tem train/val)
ls /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/
```

### Passo 3: Escolher a EstratÃ©gia

Considere:
- **Tamanho do dataset**: Pequeno â†’ EstratÃ©gia 2, Grande â†’ EstratÃ©gia 1
- **Similaridade com VGGFace2**: Similar â†’ EstratÃ©gia 1, Diferente â†’ EstratÃ©gia 2
- **Recursos computacionais**: Limitados â†’ EstratÃ©gia 2, Abundantes â†’ EstratÃ©gia 1

### Passo 4: Executar o Fine-tuning

**Exemplo completo com EstratÃ©gia 1:**

```bash
python run_finetuning.py \
    --strategy 1 \
    --pretrained_model experiments/Resnet50_vgg_cropado_CelebA/checkpoints/epoch_30.keras \
    --dataset_path /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ \
    --output_dir experiments/finetuning_strategy1 \
    --epochs 30 \
    --batch_size 64 \
    --learning_rate 0.0005 \
    --use_retinaface  # Opcional: habilita filtro RetinaFace na validaÃ§Ã£o
```

**Exemplo com EstratÃ©gia 2 (recomendado para comeÃ§ar):**

```bash
python run_finetuning.py \
    --strategy 2 \
    --pretrained_model experiments/Resnet50_vgg_cropado_CelebA/checkpoints/epoch_30.keras \
    --dataset_path /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ \
    --output_dir experiments/finetuning_strategy2 \
    --epochs 30 \
    --batch_size 64 \
    --num_layers 10 \
    --use_retinaface  # Opcional: habilita filtro RetinaFace na validaÃ§Ã£o
```

### Passo 5: Monitorar o Progresso

Durante o treinamento, vocÃª verÃ¡:
- Progresso por Ã©poca
- Loss e acurÃ¡cia de treino
- Loss e acurÃ¡cia de validaÃ§Ã£o (com ou sem filtro RetinaFace, dependendo da flag)
- Learning rate atual
- Mensagens sobre aplicaÃ§Ã£o do RetinaFace na validaÃ§Ã£o (se `--use_retinaface` estiver habilitado)

### Passo 6: Verificar Resultados

ApÃ³s o treinamento, verifique:

```bash
# Verificar checkpoints salvos
ls -lh experiments/finetuning_strategy1/checkpoints/

# Verificar logs
head experiments/finetuning_strategy1/logs/finetuning_full_fine-tuning_log.csv

# Verificar figuras geradas
ls -lh experiments/finetuning_strategy1/figures/
```

---

## ğŸ’¾ Salvamento de Modelos

### Estrutura de SaÃ­da

O script cria a seguinte estrutura:

```
experiments/finetuning_strategy1/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ finetuning_full_fine-tuning_epoch_01.keras
â”‚   â”œâ”€â”€ finetuning_full_fine-tuning_epoch_02.keras
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ finetuning_full_fine-tuning_log.csv
â”œâ”€â”€ figures/
â”‚   â””â”€â”€ finetuning_history_full_fine-tuning.png
â””â”€â”€ final_model_full_fine-tuning.keras
```

### Tipos de Modelos Salvos

1. **Checkpoints por Ã‰poca** (`checkpoints/`)
   - Um modelo salvo a cada Ã©poca
   - Ãštil para anÃ¡lise posterior
   - Permite retomar de qualquer Ã©poca

2. **Modelo Final** (`final_model_*.keras`)
   - Modelo da Ãºltima Ã©poca
   - Pronto para uso em produÃ§Ã£o
   - ContÃ©m todos os pesos otimizados

### Carregar Modelo Fine-tuned

```python
import tensorflow as tf
from src.losses.margin_losses import CosFace

# Carregar modelo
model = tf.keras.models.load_model(
    'experiments/finetuning_strategy1/final_model_full_fine-tuning.keras',
    custom_objects={'CosFace': CosFace}
)

# Usar para inferÃªncia
# ... seu cÃ³digo de inferÃªncia ...
```

### Salvar Apenas os Pesos

Se vocÃª quiser salvar apenas os pesos (mais leve):

```python
# ApÃ³s o fine-tuning
model.save_weights('experiments/finetuning_strategy1/final_weights.h5')

# Para carregar depois
model.load_weights('experiments/finetuning_strategy1/final_weights.h5')
```

---

## ğŸ“Š Monitoramento e AvaliaÃ§Ã£o

### Durante o Treinamento

O script gera automaticamente:

1. **Log CSV** (`logs/finetuning_*_log.csv`)
   - ContÃ©m: epoch, loss, accuracy, val_loss, val_accuracy, lr
   - Pode ser aberto no Excel/Pandas para anÃ¡lise

2. **GrÃ¡ficos** (`figures/finetuning_history_*.png`)
   - AcurÃ¡cia de treino e validaÃ§Ã£o
   - Loss de treino e validaÃ§Ã£o
   - Learning rate ao longo do tempo
   - Gap entre treino e validaÃ§Ã£o

### AnÃ¡lise dos Resultados

**Sinais de bom fine-tuning:**
- âœ… Loss diminuindo consistentemente
- âœ… AcurÃ¡cia aumentando
- âœ… Gap pequeno entre treino e validaÃ§Ã£o (< 5%)
- âœ… Sem overfitting (validaÃ§Ã£o acompanha treino)

**Sinais de problemas:**
- âŒ Loss nÃ£o diminui ou aumenta
- âŒ Overfitting (treino muito melhor que validaÃ§Ã£o)
- âŒ AcurÃ¡cia estagnada
- âŒ Loss com NaN
- âŒ Muitas amostras excluÃ­das pelo RetinaFace (verifique qualidade do dataset)

### ValidaÃ§Ã£o no LFW (Opcional)

ApÃ³s o fine-tuning, vocÃª pode validar no LFW:

```bash
python run_validation.py \
    --model_path experiments/finetuning_strategy1/final_model_full_fine-tuning.keras \
    --dataset_path /path/to/validation/dataset \
    --lfw_path /path/to/lfw \
    --lfw_pairs /path/to/lfw_pairs.txt
```

---

## ğŸ” Troubleshooting

### Problema: "Modelo prÃ©-treinado nÃ£o encontrado"

**SoluÃ§Ã£o:**
```bash
# Verificar se o caminho estÃ¡ correto
ls -lh experiments/Resnet50_vgg_cropado_CelebA/checkpoints/

# Usar caminho absoluto
python run_finetuning.py \
    --pretrained_model /caminho/absoluto/para/modelo.keras \
    ...
```

### Problema: "Dataset nÃ£o encontrado"

**SoluÃ§Ã£o:**
```bash
# Verificar permissÃµes
ls -lh /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/

# Verificar estrutura
find /dados/datasets/aligned_112x112/vggface2_dataset_all_splits_merged/ -type f | head -10
```

### Problema: "NÃºmero de classes diferente"

**SoluÃ§Ã£o:**
O script ajusta automaticamente o nÃºmero de classes. Se houver erro:
- Verifique se o dataset tem pelo menos 2 classes
- Verifique se as pastas estÃ£o organizadas corretamente

### Problema: "Out of Memory (OOM)"

**SoluÃ§Ãµes:**
1. Reduzir batch size:
   ```bash
   --batch_size 32  # ou 16, ou 8
   ```

2. Usar EstratÃ©gia 2 (menos parÃ¢metros):
   ```bash
   --strategy 2 --num_layers 5
   ```

3. Reduzir tamanho da imagem (modificar config se necessÃ¡rio)

### Problema: "Loss nÃ£o diminui"

**SoluÃ§Ãµes:**
1. Aumentar learning rate:
   ```bash
   --learning_rate 0.001
   ```

2. Verificar se o dataset estÃ¡ correto
3. Verificar se o modelo prÃ©-treinado estÃ¡ carregado corretamente
4. Tentar EstratÃ©gia 1 (mais flexÃ­vel)

### Problema: "Overfitting"

**SoluÃ§Ãµes:**
1. Usar EstratÃ©gia 2:
   ```bash
   --strategy 2 --num_layers 5
   ```

2. Reduzir learning rate:
   ```bash
   --learning_rate 0.0001
   ```

3. Adicionar mais dados de treinamento
4. Usar data augmentation (jÃ¡ incluÃ­do no pipeline)

### Problema: "Muitas amostras excluÃ­das pelo RetinaFace"

**Sintomas:**
- Mensagem indicando muitas exclusÃµes durante a validaÃ§Ã£o
- Dataset de validaÃ§Ã£o muito pequeno apÃ³s filtro

**SoluÃ§Ãµes:**
1. Verificar qualidade das imagens no dataset:
   ```bash
   # Verificar algumas imagens manualmente
   find /dados/datasets/.../val -name "*.jpg" | head -10 | xargs -I {} open {}
   ```

2. Verificar se as imagens estÃ£o alinhadas corretamente
3. Considerar prÃ©-processar o dataset antes do fine-tuning
4. Verificar se o RetinaFace estÃ¡ funcionando corretamente:
   - Teste com algumas imagens manualmente
   - Verifique logs de erro do RetinaFace

**Nota:** Algumas exclusÃµes sÃ£o normais, especialmente se o dataset contÃ©m imagens de baixa qualidade ou sem rosto visÃ­vel.

---

## ğŸ“ˆ RecomendaÃ§Ãµes Finais

### Para Iniciantes

1. Comece com **EstratÃ©gia 2** (`--strategy 2 --num_layers 10`)
2. Use `--epochs 20` para testes iniciais
3. Monitore os grÃ¡ficos gerados
4. Ajuste `--num_layers` baseado nos resultados

### Para Experientes

1. Experimente todas as 3 estratÃ©gias
2. Compare resultados usando os logs CSV
3. Ajuste learning rates baseado no dataset
4. Considere treinar em mÃºltiplas fases

### Boas PrÃ¡ticas

- âœ… Sempre valide em um conjunto de teste separado
- âœ… Salve checkpoints frequentes
- âœ… Monitore overfitting
- âœ… Documente os parÃ¢metros usados
- âœ… Compare diferentes estratÃ©gias no mesmo dataset
- âœ… Verifique a qualidade do dataset antes do fine-tuning
- âœ… Monitore quantas amostras sÃ£o excluÃ­das pelo RetinaFace na validaÃ§Ã£o
- âœ… Use o filtro RetinaFace para garantir validaÃ§Ã£o com imagens vÃ¡lidas

---

## ğŸ“š ReferÃªncias

- [CosFace Paper](https://arxiv.org/abs/1801.09414)
- [Transfer Learning Guide](https://www.tensorflow.org/guide/keras/transfer_learning)
- [Fine-tuning Best Practices](https://cs231n.github.io/transfer-learning/)

---

## ğŸ¤ Suporte

Para problemas ou dÃºvidas:
1. Verifique os logs em `experiments/finetuning_*/logs/`
2. Verifique os grÃ¡ficos em `experiments/finetuning_*/figures/`
3. Consulte a seÃ§Ã£o de Troubleshooting acima

---

**Ãšltima atualizaÃ§Ã£o:** 2024

---

## ğŸ”„ HistÃ³rico de MudanÃ§as

### VersÃ£o Atual

- âœ… **RetinaFace Opcional na ValidaÃ§Ã£o**: Filtro de RetinaFace pode ser habilitado com `--use_retinaface`
- âœ… **PolÃ­tica de ExclusÃ£o**: Amostras sem detecÃ§Ã£o de rosto sÃ£o automaticamente excluÃ­das da validaÃ§Ã£o (quando habilitado)
- âœ… **Processamento Eficiente**: RetinaFace Ã© carregado uma Ãºnica vez e reutilizado
- âœ… **Apenas na ValidaÃ§Ã£o**: Filtro aplicado apenas na validaÃ§Ã£o, nÃ£o afeta o treinamento
- âœ… **Controle FlexÃ­vel**: UsuÃ¡rio decide quando usar o filtro atravÃ©s da flag `--use_retinaface`

